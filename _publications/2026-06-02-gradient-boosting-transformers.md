---
title: "Tricks and Plug-ins for Gradient Boosting for Transformers"
collection: publications
category: manuscripts
permalink: /publication/2026-gradient-boosting-transformers
excerpt: 'Introduced a sub-sequence selection strategy for transformer boosting, improving computational efficiency and enabling scaling to longer sequences.'
date: 2026-06-01
venue: 'Under review at ICPR 2026'
paperurl: 'https://arxiv.org/abs/2508.02924'
citation: 'Biyi Fang, Truong Vo, Jean Utke, and Diego Klabjan. (2026). Tricks and Plug-ins for Gradient Boosting for Transformers. <i>Under review at the 27th International Conference on Pattern Recognition (ICPR 2026)</i>.'
---

This paper extends gradient boosting techniques to transformer architectures with novel sub-sequence selection strategies.

[Download paper here](https://arxiv.org/abs/2508.02924)
