---
title: "Tricks and Plug-ins for Gradient Boosting for Transformers"
collection: publications
category: manuscripts
permalink: /publication/2026-gradient-boosting-transformers
excerpt: 'Introduced a sub-sequence selection strategy for transformer boosting, improving computational efficiency and enabling scaling to longer sequences.'
date: 2026-06-01
venue: 'Under review at ICPR 2026'
paperurl: 'https://arxiv.org/abs/2508.02924'
citation: 'Biyi Fang, Truong Vo, Jean Utke, and Diego Klabjan. (2026). Tricks and Plug-ins for Gradient Boosting for Transformers. <i>Under review at the 27th International Conference on Pattern Recognition (ICPR 2026)</i>.'
---

Transformer architectures dominate modern NLP but often demand heavy computational resources and intricate hyperparameter tuning. To mitigate these challenges, we propose a novel framework, BoostTransformer, that augments transformers with boosting principles through subgrid token selection and importance-weighted sampling. Our method incorporates a least square boosting objective directly into the transformer pipeline, enabling more efficient training and improved performance. Across multiple fine-grained text classification benchmarks, BoostTransformer demonstrates both faster convergence and higher accuracy, surpassing standard transformers while minimizing architectural search overhead.


[Download paper here](https://arxiv.org/abs/2508.02924)
